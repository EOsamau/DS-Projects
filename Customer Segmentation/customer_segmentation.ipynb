{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('online_retail.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has many missing customer IDs and most of these records are related to purchases with the same invoice date. A prudent measure to treat this missing data would be to create pseudo_IDs for the missing data however it will not be realistic as there are multiple purchases made by a customer. We will have to resort to eliminating theses rows from the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()\n",
    "data.isna().sum()\n",
    "data = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "data['InvoiceDate'] = pd.to_datetime(data['InvoiceDate'])\n",
    "\n",
    "#Total purchases by customer\n",
    "data = data.groupby('CustomerID').agg(\n",
    "                            {'Quantity': 'sum',\n",
    "                            'UnitPrice': 'mean',\n",
    "                            'InvoiceNo': 'count',\n",
    "                            'InvoiceDate': 'max'\n",
    "                            })\n",
    "\n",
    "data['TotalSpend'] = data['Quantity'] * data['UnitPrice']\n",
    "\n",
    "latest_date = data['InvoiceDate'].max()\n",
    "\n",
    "data['Recency'] = (latest_date  - data['InvoiceDate']).dt.days\n",
    "\n",
    "customer_data = data.rename(columns={'InvoiceNo': 'Frequency'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaled_data = scaler.fit_transform(customer_data[['Quantity','Frequency','TotalSpend','Recency']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering\n",
    "\n",
    "### Randomly choose a K = 5 to test the clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=5, random_state=42) #Randomly choosing a number of Clusters.\n",
    "customer_data['Cluster'] = kmeans.fit_predict(scaled_data)\n",
    "\n",
    "customer_data.groupby('Cluster').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the optimal K using the elbow method by calculating the Within-Cluster Sum of Squares (WCSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wcss = []\n",
    "K_range = range(2, 10)  \n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(scaled_data)  \n",
    "    wcss.append(kmeans.inertia_)  # Inertia = WCSS metric in K-Means\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(K_range, wcss, marker='o', linestyle='-', color='r')\n",
    "\n",
    "\n",
    "plt.title('Elbow Method for Optimal K', fontsize=14)\n",
    "plt.xlabel('Number of Clusters (K)', fontsize=12)\n",
    "plt.ylabel('Within-Cluster Sum of Squares (WCSS)', fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.savefig('Elbow_Optimal_K.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The elbow appears to be at K = 4. Up until K=4, the WCSS decreases significantly, but after that, the reduction in WCSS becomes more gradual. The optimal K for customer data is 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=4, random_state=42) #Randomly choosing a number of Clusters.\n",
    "customer_data['Cluster'] = kmeans.fit_predict(scaled_data)\n",
    "\n",
    "kmeans_clusters = customer_data.groupby('Cluster').mean()\n",
    "\n",
    "kmeans_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cluster centroids for the features show the average behavior of customers in each cluster. Cluster 1 shows customers who have purchased in large quantities (21,475) and at high unit prices (135.42), indicating that they may be high-value customers.\n",
    "Cluster 2, on the other hand, has much lower quantities and unit prices, suggesting these customers may be more occasional or lower-value buyers. Cluster 0 may represent small-scale buyers, and Cliuster 3 may represent large-scale buyers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "\n",
    "Using the 4-means Clustering as a benchmark to compare to other algorithms like Agglomerative Clustering, and DBSCAN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrapping to evaluate the Variance Ratio of the three algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "from sklearn.utils import resample\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "\n",
    "def process_bootstrap(i, scaled_data):\n",
    "    np.random.seed(i)\n",
    "    X_bootstrap = resample(scaled_data, replace=True, n_samples=len(scaled_data))\n",
    "    \n",
    "    results = {\n",
    "        'kmeans': {'score': 0, 'params': {}},\n",
    "        'agglo': {'score': 0, 'params': {}},\n",
    "        'dbscan': {'score': 0, 'params': {}}\n",
    "    }\n",
    "    \n",
    "   \n",
    "    k = 4  # optimal k from elbow method\n",
    "    kmeans = KMeans(n_clusters=k, random_state=i, n_init=10)\n",
    "    labels = kmeans.fit_predict(X_bootstrap)\n",
    "    score = calinski_harabasz_score(X_bootstrap, labels)\n",
    "    results['kmeans'] = {'score': score, 'params': {'n_clusters': k}}\n",
    "    \n",
    "    # Agglomerative Clustering\n",
    "    for k in range(2, 10):\n",
    "        agglo = AgglomerativeClustering(n_clusters=k)\n",
    "        labels = agglo.fit_predict(X_bootstrap)\n",
    "        score = calinski_harabasz_score(X_bootstrap, labels)\n",
    "        if score > results['agglo']['score']:\n",
    "            results['agglo'] = {'score': score, 'params': {'n_clusters': k}}\n",
    "    \n",
    "    # DBSCAN\n",
    "    eps_range = np.linspace(0.1, 1.5, 10)\n",
    "    min_samples_range = range(3, 10)\n",
    "    for eps in eps_range:\n",
    "        for min_samples in min_samples_range:\n",
    "            dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "            labels = dbscan.fit_predict(X_bootstrap)\n",
    "            if len(set(labels)) > 1:\n",
    "                score = calinski_harabasz_score(X_bootstrap, labels)\n",
    "                if score > results['dbscan']['score']:\n",
    "                    results['dbscan'] = {'score': score, 'params': {'eps': eps, 'min_samples': min_samples}}\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Executing the models and determining the best parameters and scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "n_bootstraps = 100  \n",
    "results = Parallel(n_jobs=-1)(delayed(process_bootstrap)(i, scaled_data) for i in range(n_bootstraps))\n",
    "\n",
    "best_params = {\n",
    "    'kmeans': {'score': 0, 'params': {}},\n",
    "    'agglo': {'score': 0, 'params': {}},\n",
    "    'dbscan': {'score': 0, 'params': {}}\n",
    "}\n",
    "\n",
    "for r in results:\n",
    "    for method in best_params:\n",
    "        if r[method]['score'] > best_params[method]['score']:\n",
    "            best_params[method] = r[method]\n",
    "\n",
    "print(\"Best parameters:\")\n",
    "for method, params in best_params.items():\n",
    "    print(f\"{method}: {params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Algorithm benchmarking with K-means "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_algorithm = max(best_params.items(), key=lambda x: x[1]['score'])\n",
    "\n",
    "\n",
    "if best_algorithm[0] == 'kmeans':\n",
    "    final_model = KMeans(n_clusters=best_params['kmeans']['params']['n_clusters'], random_state=42)\n",
    "elif best_algorithm[0] == 'agglo':\n",
    "    final_model = AgglomerativeClustering(n_clusters=best_params['agglo']['params']['n_clusters'])\n",
    "else:  \n",
    "    final_model = DBSCAN(eps=best_params['dbscan']['params']['eps'], \n",
    "                         min_samples=best_params['dbscan']['params']['min_samples'])\n",
    "\n",
    "\n",
    "# Best model using CALINSKI_HARABASZ SCORE\n",
    "print(f\"Best algorithm: {best_algorithm[0]}\")\n",
    "print(f\"Best parameters: {best_params[best_algorithm[0]]['params']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_clustering_performance(results):\n",
    "    n_bootstraps = len(results)\n",
    "    \n",
    "\n",
    "    scores_kmeans = [r['kmeans']['score'] for r in results]\n",
    "    scores_agglomerative = [r['agglo']['score'] for r in results]\n",
    "    scores_dbscan = [r['dbscan']['score'] for r in results]\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "   \n",
    "    plt.plot(range(n_bootstraps), scores_kmeans, label='K-Means', marker='o', linestyle='-', markersize=4)\n",
    "    plt.plot(range(n_bootstraps), scores_agglomerative, label='Agglomerative', marker='s', linestyle='-', markersize=4)\n",
    "    plt.plot(range(n_bootstraps), scores_dbscan, label='DBSCAN', marker='^', linestyle='-', markersize=4)\n",
    "\n",
    " \n",
    "    plt.xlabel('Bootstrap Iteration')\n",
    "    plt.ylabel('Calinski-Harabasz Score')\n",
    "    plt.title('Model Performance across Bootstraps')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(False)\n",
    "\n",
    "   #Annotations\n",
    "    mean_kmeans = np.mean(scores_kmeans)\n",
    "    mean_agglomerative = np.mean(scores_agglomerative)\n",
    "    mean_dbscan = np.mean(scores_dbscan)\n",
    "\n",
    "    plt.annotate(f'K-Means Mean: {mean_kmeans:.2f}', xy=(0.20, 0.95), xycoords='axes fraction')\n",
    "    plt.annotate(f'Agglomerative Mean: {mean_agglomerative:.2f}', xy=(0.20, 0.90), xycoords='axes fraction')\n",
    "    plt.annotate(f'DBSCAN Mean: {mean_dbscan:.2f}', xy=(0.20, 0.85), xycoords='axes fraction')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Model_Performance_Across_Bootstraps.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_clustering_performance(results)\n",
    "\n",
    "\n",
    "for algorithm in ['kmeans', 'agglo', 'dbscan']:\n",
    "    best_score = max(r[algorithm]['score'] for r in results)\n",
    "    best_params = next(r[algorithm]['params'] for r in results if r[algorithm]['score'] == best_score)\n",
    "    print(f\"\\nBest {algorithm.capitalize()} parameters:\")\n",
    "    print(f\"Score: {best_score:.2f}\")\n",
    "    print(f\"Parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the Agglomerative algorithm achieved the highest Calinski-Harabasz score, its mean is fairly comparable to K-Means, which uses fewer clusters. Given that K-Means offers better interpretability and simplicity, it may be a preferable choice despite the slight trade-off in score. We proceed to use PCA for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "cluster_distribution = customer_data['Cluster'].value_counts(normalize=True).reset_index()\n",
    "cluster_distribution.columns = ['Cluster', 'Proportion']\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.barplot(x='Cluster', y='Proportion', data=cluster_distribution)\n",
    "\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Proportion of Customers')\n",
    "plt.title('Distribution of Customers by Cluster')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Labeling for 4-Means Clustering\n",
    "\n",
    "Based on the centroids of these clusters:\n",
    "\n",
    "Cluster 0: Small Retailers: Customers in this cluster buy a moderate quantity of goods (1123 units on average) at a relatively low unit price (4.09). Their frequency of purchases is high, and they spend a significant amount overall $3,771. These customers are likely small businesses that make regular, bulk purchases but operate at a smaller scale.\n",
    "\n",
    "Cluster 1: Large Wholesalers: This group has a significantly high total spend (~ $251,969) and makes massive bulk purchases (60,307 units on average), which suggests these are likely large wholesale distributors. They operate with relatively low recency (3.73 days), indicating frequent transactions, typical of large wholesale operations.\n",
    "\n",
    "Cluster 2: Boutique Shops: This cluster has lower quantity purchases (259 units) but pays a higher unit price (19.37), possibly indicating specialty or  businesses purchasing premium products. They are less frequent buyers, with a higher recency, which suggests occasional but expensive purchases.\n",
    "\n",
    "Cluster 3: Corporate Clients: These customers spend the most ($2.35 million) and buy very large quantities (21,475 units), often at a high price ($135.42). Their purchase frequency is not as high as Cluster 1, but their overall spending suggests they are corporate clients with large, infrequent orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cluster distribution\n",
    "cluster_distribution = customer_data['Cluster'].value_counts(normalize=True).reset_index()\n",
    "cluster_distribution.columns = ['Cluster', 'Proportion']\n",
    "\n",
    "\n",
    "industry_groups = {\n",
    "    0: 'Small Retailers',\n",
    "    1: 'Large Wholesalers',\n",
    "    2: 'Specialty Shops',\n",
    "    3: 'Corporate Clients'\n",
    "}\n",
    "\n",
    "\n",
    "cluster_distribution['Cluster'] = cluster_distribution['Cluster'].map(industry_groups)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(7, 6))\n",
    "sns.barplot(x='Cluster', y='Proportion', data=cluster_distribution)\n",
    "\n",
    "plt.xlabel('Industry Group')\n",
    "plt.ylabel('Proportion of Customers')\n",
    "plt.title('Distribution of Customers by Industry Group')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the Clusters using PCA (2-Dimensional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def plot_pca_clusters(scaled_data, cluster_labels, best_algorithm):\n",
    "    # Reducing the dimensionality to 2D using PCA\n",
    "    pca = PCA(n_components= 2)\n",
    "    pca_data = pca.fit_transform(scaled_data)\n",
    "\n",
    "\n",
    "    pca_df = pd.DataFrame(data=pca_data, columns=['PC1', 'PC2'])\n",
    "    pca_df['Cluster'] = cluster_labels\n",
    "\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(data=pca_df, x='PC1', y='PC2', hue='Cluster', palette='deep', s=60, alpha=0.7)\n",
    "\n",
    "    plt.title(f'Customer Segmentation using {best_algorithm} (PCA Reduced Dimensions)')\n",
    "    plt.xlabel(f'Principal Component 1 ({pca.explained_variance_ratio_[0]:.2%} variance explained)')\n",
    "    plt.ylabel(f'Principal Component 2 ({pca.explained_variance_ratio_[1]:.2%} variance explained)')\n",
    "\n",
    "\n",
    "    plt.legend(title='Cluster', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "  \n",
    "    total_var = pca.explained_variance_ratio_.sum()\n",
    "    plt.annotate(f'Total variance explained: {total_var:.2%}', \n",
    "                 xy=(0.02, 0.98), xycoords='axes fraction',\n",
    "                 fontsize=10, ha='left', va='top')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print top features contributing to each principal component\n",
    "    print(\"\\nTop features contributing to principal components:\")\n",
    "    feature_importance = pd.DataFrame(\n",
    "        pca.components_.T,\n",
    "        columns=[f'PC{i+1}' for i in range(2)],\n",
    "        index=scaled_data.columns if hasattr(scaled_data, 'columns') else [f'Feature {i}' for i in range(scaled_data.shape[1])]\n",
    "    ).abs()\n",
    "\n",
    "    for i in range(2):\n",
    "        print(f\"\\nPC{i+1} top contributors:\")\n",
    "        print(feature_importance[f'PC{i+1}'].nlargest(5))\n",
    "\n",
    "\n",
    "\n",
    "plot_pca_clusters(scaled_data, cluster_labels, best_algorithm[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pca_kmeans_analysis(scaled_data, n_components=2, n_clusters=4, random_state=42):\n",
    "    # Perform PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca_data = pca.fit_transform(scaled_data)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)\n",
    "    cluster_labels = kmeans.fit_predict(scaled_data)\n",
    "    \n",
    "    ch_score = calinski_harabasz_score(pca_data, cluster_labels)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    scatter = plt.scatter(pca_data[:, 0], pca_data[:, 1], c=cluster_labels, cmap='viridis', alpha=0.7)\n",
    "    plt.colorbar(scatter)\n",
    "    \n",
    "    plt.title(f'K-means Clustering on PCA-reduced Data (k={n_clusters})')\n",
    "    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance explained)')\n",
    "    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance explained)')\n",
    "    \n",
    "    centers = kmeans.cluster_centers_\n",
    "    plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.75, marker='X')\n",
    "    \n",
    "\n",
    "    total_var = pca.explained_variance_ratio_.sum()\n",
    "    plt.annotate(f'Total variance explained: {total_var:.2%}', \n",
    "                 xy=(0.02, 0.98), xycoords='axes fraction',\n",
    "                 fontsize=10, ha='left', va='top')\n",
    "    plt.annotate(f'Calinski-Harabasz Score: {ch_score:.2f}', \n",
    "                 xy=(0.02, 0.94), xycoords='axes fraction',\n",
    "                 fontsize=10, ha='left', va='top')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    " \n",
    "    print(\"\\nTop features contributing to principal components:\")\n",
    "    feature_importance = pd.DataFrame(\n",
    "        pca.components_.T,\n",
    "        columns=[f'PC{i+1}' for i in range(n_components)],\n",
    "        index=scaled_data.columns if hasattr(scaled_data, 'columns') else [f'Feature {i}' for i in range(scaled_data.shape[1])]\n",
    "    ).abs()\n",
    "\n",
    "    for i in range(n_components):\n",
    "        print(f\"\\nPC{i+1} top contributors:\")\n",
    "        print(feature_importance[f'PC{i+1}'].nlargest(5))\n",
    "    \n",
    "    return pca, kmeans, cluster_labels, ch_score\n",
    "\n",
    "\n",
    "n_clusters = 4\n",
    "\n",
    "pca, kmeans, cluster_labels, ch_score = pca_kmeans_analysis(scaled_data, n_components=2, n_clusters=n_clusters)\n",
    "\n",
    "customer_data['Cluster'] = cluster_labels\n",
    "\n",
    "print(f\"\\nCalinski-Harabasz Score: {ch_score:.2f}\")\n",
    "print(f\"Number of samples in each cluster:\\n{pd.Series(cluster_labels).value_counts().sort_index()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_data['Cluster'] = cluster_labels\n",
    "\n",
    "industry_groups = {\n",
    "    0: 'Small Retailers',\n",
    "    1: 'Large Wholesalers',\n",
    "    2: 'Specialty Shops',\n",
    "    3: 'Corporate Clients'\n",
    "}\n",
    "\n",
    "customer_data['Cluster Description'] = customer_data['Cluster'].map(industry_groups)\n",
    "\n",
    "\n",
    "output_data = customer_data[['Cluster', 'Cluster Description']]\n",
    "\n",
    "output_data.to_csv('2DPCA_4MEANS_grouping.csv', index=False)\n",
    "\n",
    "print(\"CSV file created with customer IDs and their assigned clusters.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Calinski-Harabasz score suggests good cluster quality, but the extremely small cluster sizes (Cluster 1 and Cluster 3) may indicate potential outliers or sparse customer groups. The PCA diagrams demonstrate that K-means clustering effectively groups the data similarly to Agglomerative Clustering, but with fewer clusters, contributing to improved model simplicity. \n",
    "\n",
    "The centroids appear to be reasonably well-placed for the visible data distribution. The overlap between the purple and green clusters suggests that these might not be truly distinct clusters in the original high-dimensional space. \n",
    "\n",
    "Therefore we can experiment increasing the dimensions to a 3D-PCA from a 2D-PCA, and selecting the number of clusters with across the silhouette and the Calinski HB Scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing   3-D PCA with K =2 (Based on Silhouette scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import calinski_harabasz_score, silhouette_score\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "\n",
    "def improved_pca_kmeans_analysis(scaled_data, max_clusters=10, n_components=3, random_state=42, outlier_fraction=0.1):\n",
    "    # Outlier detection\n",
    "    outlier_detector = EllipticEnvelope(contamination=outlier_fraction, random_state=random_state)\n",
    "    outlier_labels = outlier_detector.fit_predict(scaled_data)\n",
    "    inlier_mask = outlier_labels == 1\n",
    "    inlier_data = scaled_data[inlier_mask]\n",
    "\n",
    "    # Perform PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca_data = pca.fit_transform(inlier_data)\n",
    "\n",
    "    # Elbow method\n",
    "    inertias = []\n",
    "    ch_scores = []\n",
    "    silhouette_scores = []\n",
    "    \n",
    "    for k in range(2, max_clusters + 1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=random_state)\n",
    "        cluster_labels = kmeans.fit_predict(pca_data)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "        ch_scores.append(calinski_harabasz_score(pca_data, cluster_labels))\n",
    "        silhouette_scores.append(silhouette_score(pca_data, cluster_labels))\n",
    "\n",
    "    # Plot elbow curve\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(131)\n",
    "    plt.plot(range(2, max_clusters + 1), inertias, marker='o')\n",
    "    plt.title('Elbow Method')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Inertia')\n",
    "\n",
    "    plt.subplot(132)\n",
    "    plt.plot(range(2, max_clusters + 1), ch_scores, marker='o')\n",
    "    plt.title('Calinski-Harabasz Score')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('CH Score')\n",
    "\n",
    "    plt.subplot(133)\n",
    "    plt.plot(range(2, max_clusters + 1), silhouette_scores, marker='o')\n",
    "    plt.title('Silhouette Score')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Select optimal number of clusters (you can adjust this based on the plots)\n",
    "    optimal_clusters = silhouette_scores.index(max(silhouette_scores)) + 2\n",
    "\n",
    "    # Perform final clustering\n",
    "    kmeans = KMeans(n_clusters=optimal_clusters, random_state=random_state)\n",
    "    cluster_labels = kmeans.fit_predict(pca_data)\n",
    "\n",
    "    # Visualize results\n",
    "    fig = plt.figure(figsize=(12, 7))\n",
    "    \n",
    "    # 2D plot\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    scatter = ax1.scatter(pca_data[:, 0], pca_data[:, 1], c=cluster_labels, cmap='viridis', alpha=0.7)\n",
    "    ax1.set_title(f'2D PCA-reduced Data (k={optimal_clusters})')\n",
    "    ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "    ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "    plt.colorbar(scatter)\n",
    "\n",
    "    # 3D plot\n",
    "    ax2 = fig.add_subplot(122, projection='3d')\n",
    "    scatter = ax2.scatter(pca_data[:, 0], pca_data[:, 1], pca_data[:, 2], c=cluster_labels, cmap='viridis', alpha=0.7)\n",
    "    ax2.set_title(f'3D PCA-reduced Data (k={optimal_clusters})')\n",
    "    ax2.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "    ax2.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "    ax2.set_zlabel(f'PC3 ({pca.explained_variance_ratio_[2]:.2%} variance)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Feature importance analysis\n",
    "    feature_importance = pd.DataFrame(\n",
    "        pca.components_.T,\n",
    "        columns=[f'PC{i+1}' for i in range(n_components)],\n",
    "        index=scaled_data.columns if hasattr(scaled_data, 'columns') else [f'Feature {i}' for i in range(scaled_data.shape[1])]\n",
    "    ).abs()\n",
    "\n",
    "    print(\"\\nTop features contributing to principal components:\")\n",
    "    for i in range(n_components):\n",
    "        print(f\"\\nPC{i+1} top contributors:\")\n",
    "        print(feature_importance[f'PC{i+1}'].nlargest(5))\n",
    "\n",
    "    # Create full_cluster_labels with the same length as the original data\n",
    "    full_cluster_labels = np.full(len(scaled_data), -1)  # -1 for outliers\n",
    "    full_cluster_labels[inlier_mask] = cluster_labels\n",
    "\n",
    "    return pca, kmeans, full_cluster_labels, feature_importance, inlier_mask\n",
    "\n",
    "# Usage\n",
    "n_components = 3\n",
    "pca, kmeans, cluster_labels, feature_importance, inlier_mask = improved_pca_kmeans_analysis(scaled_data, n_components=n_components)\n",
    "\n",
    "# Add cluster labels to original data\n",
    "customer_data['Cluster'] = cluster_labels\n",
    "\n",
    "print(f\"\\nNumber of samples in each cluster:\\n{pd.Series(cluster_labels).value_counts().sort_index()}\")\n",
    "print(f\"\\nNumber of outliers: {sum(cluster_labels == -1)}\")\n",
    "print(f\"\\nSilhouette Score: {silhouette_score(pca.transform(scaled_data[inlier_mask]), cluster_labels[inlier_mask]):.4f}\")\n",
    "print(f\"Calinski-Harabasz Score: {calinski_harabasz_score(pca.transform(scaled_data[inlier_mask]), cluster_labels[inlier_mask]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new analysis suggests 2 clusters (k=2) as optimal, compared to the previous 4 clusters for K-means. The new 2D and 3D PCA plots show a clearer separation between two main groups with minimal overlap.\n",
    "\n",
    "The total variance explained by the first two components is now 98.81%, compared to 68.72% previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New customer segmentation with 2-Clusters\n",
    "\n",
    "While the centroids of the 4-Means Cluster provided real life industry insights into the customer behaviour, There appears to be fewer outliers in this new clustering, as most points seem to belong clearly to one of the two clusters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_2 = KMeans(n_clusters= 2, random_state=42) #Randomly choosing a number of Clusters.\n",
    "customer_data_2 = customer_data.drop(['Cluster', 'Cluster Description'], axis=1)\n",
    "\n",
    "customer_data_2['Cluster_2'] = kmeans_2.fit_predict(scaled_data)\n",
    "\n",
    "kmeans_clusters = customer_data_2.groupby('Cluster_2').mean()\n",
    "\n",
    "kmeans_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster 0: Low-Spend, Inactive Buyers – These are customers who make smaller purchases less frequently and have not been active recently. Their total spend is relatively low, and they may represent small or less engaged customers.\n",
    "\n",
    "Cluster 1: High-Spend, Active Buyers – These are high-value customers who purchase frequently, spend more, and have made recent transactions. They may represent key clients or loyal customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "industry_groups_2 = {\n",
    "    -1: 'Random Buyers',\n",
    "    0: 'Low-Spend, Inactive Buyers',\n",
    "    1: 'High-Spend, Active Buyers',\n",
    "}\n",
    "\n",
    "customer_data_2['Cluster Description'] = customer_data_2['Cluster_2'].map(industry_groups_2)\n",
    "\n",
    "\n",
    "output_data_2 = customer_data[['Cluster_2', 'Cluster Description']]\n",
    "\n",
    "output_data_2.drop(columns=['Cluster Description'])\n",
    "\n",
    "output_data_2.to_csv('3DPCA_2MEANS_grouping.csv', index=False)\n",
    "\n",
    "output_data_2.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_data_2[['Cluster Description']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "import joblib  \n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()), \n",
    "    ('kmeans', KMeans(n_clusters=2, random_state=42))\n",
    "])\n",
    "\n",
    "pipeline.fit(scaled_data)\n",
    "\n",
    "joblib.dump(pipeline, 'kmeans_clustering_pipeline.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
